{
  "general": {

    "data_dir": {
      "value": "data/conll03",
      "default": null,
      "desc": "The input data dir. Should contain the .tsv files (or other data files) for the task."
    },

    "bert_model": {
      "value": "bert-base-cased-de-v0-1",
      "default": null,
      "desc": "Bert pre-trained model selected in the list: bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese."
    },

    "task_name": {
      "value": null,
      "default": "germeval18",
      "desc": "The name of the task to train."
    },

    "output_dir": {
      "value": null,
      "default": "output",
      "desc": "The output directory where the model predictions and checkpoints will be written."
    },

    "cache_dir": {
      "value": null,
      "default": "",
      "desc": "Where do you want to store the pre-trained models downloaded from s3"
    },

    "max_seq_length": {
      "value": null,
      "default": 128,
      "desc": "The maximum total input sequence length after WordPiece tokenization. Sequences longer than this will be truncated, and sequences shorter than this will be padded."
    },

    "do_train": {
      "value": true,
      "default": false,
      "desc": "Whether to run training."
    },

    "do_eval": {
      "value": true,
      "default": false,
      "desc": "Whether to run eval on the dev set."
    },

    "do_lower_case": {
      "value": null,
      "default": false,
      "desc": "Set this flag if you are using an uncased model."
    },

    "train_batch_size": {
      "value": 64,
      "default": 32,
      "desc": "Total batch size for training."
    },

    "learning_rate": {
      "value": null,
      "default": 2e-5,
      "desc": "The initial learning rate for Adam."
    },

    "num_train_epochs":  {
      "value": null,
      "default": 3.0,
      "desc": "Total number of training epochs to perform."
    },

    "warmup_proportion":  {
      "value": null,
      "default": 0.1,
      "desc": "Proportion of training to perform linear learning rate warmup for. E.g., 0.1 = 10%% of training."
    },

    "no_cuda":  {
      "value": true,
      "default": false,
      "desc": "Whether not to use CUDA when available."
    },

    "local_rank":  {
      "value": null,
      "default": -1,
      "desc": "local_rank for distributed training on gpus."
    },

    "seed":  {
      "value": null,
      "default": 42,
      "desc": "random seed for initialization"
    },

    "gradient_accumulation_steps":  {
      "value": null,
      "default": 1,
      "desc": "Number of updates steps to accumulate before performing a backward/update pass."
    },

    "fp16":  {
      "value": null,
      "default": false,
      "desc": "Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True. 0 (default value): dynamic loss scaling. Positive power of 2: static loss scaling value."
    },

    "loss_scale":  {
      "value": null,
      "default": 0,
      "desc": "Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True. 0 (default value): dynamic loss scaling. Positive power of 2: static loss scaling value."
    },

    "server_ip":  {
      "value": null,
      "default": "",
      "desc": "Can be used for distant debugging."
    },

    "server_port":  {
      "value": null,
      "default": "",
      "desc": "Can be used for distant debugging."
    },

    "eval_every":  {
      "value": null,
      "default": 30,
      "desc": "Steps per training loop required for evaluation on dev set. Only used when do_eval set to true."
    },
    "eval_batch_size": {
      "value": 8,
      "default": 32,
      "desc": "Total batch size for eval."
    }
  },

  "task": {

  }
}


